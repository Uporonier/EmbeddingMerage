from pathlib import Path
from transformers import AutoModelForCausalLM, AutoTokenizer
from datasets import load_dataset
import torch
import torch.nn as nn
import torch.nn.functional as F
import math
from tqdm.auto import tqdm
import numpy as np

# ==================== 配置 ====================
class Config:
    # 模型路径
    model_1b_path = '/content/EmbeddingMerage/models/llama-1b'
    model_3b_path = '/content/EmbeddingMerage/models/llama-3b'
    projector_path = '/content/EmbeddingMerage/best_projector.pt'
    output_dir = '/content/EmbeddingMerage/models/best-weight-meraged'
    
    # 评估参数
    eval_dataset = "wikitext-2-v1"  # 使用完整测试集
    print_interval = 100  # 每处理100个样本打印一次进度
    
    # 融合参数
    test_alphas = [0, 0.1, 0.2, 0.3, 0.5, 0.7, 1.0]  # 待测试的alpha值
    
    # 投影层参数
    proj_hidden_dim = 4096  # 需与训练时一致

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ==================== 工具函数 ====================
def load_base_models():
    """加载基础模型和分词器"""
    print("Loading base models...")
    model_1b = AutoModelForCausalLM.from_pretrained(
        Config.model_1b_path,
        device_map="auto",
        torch_dtype=torch.float16
    )
    model_3b = AutoModelForCausalLM.from_pretrained(
        Config.model_3b_path,
        device_map="auto",
        torch_dtype=torch.float16
    )
    tokenizer = AutoTokenizer.from_pretrained(Config.model_1b_path)
    return model_1b, model_3b, tokenizer

def get_normalized_embeddings(model):
    """提取并归一化嵌入层"""
    emb = model.model.embed_tokens.weight.data.clone()
    emb = emb.to(device).float()
    return F.normalize(emb, p=2, dim=1)

class TokenProjector(nn.Module):
    """投影层定义（必须与训练时一致）"""
    def __init__(self, in_dim, out_dim):
        super().__init__()
        self.proj = nn.Sequential(
            nn.Linear(in_dim, Config.proj_hidden_dim),
            nn.GELU(),
            nn.LayerNorm(Config.proj_hidden_dim),
            nn.Linear(Config.proj_hidden_dim, out_dim),
            nn.LayerNorm(out_dim)
        )
    
    def forward(self, x):
        return F.normalize(self.proj(x), dim=-1, eps=1e-8)

def evaluate_full_ppl(model, tokenizer, dataset):
    """评估完整测试集的困惑度"""
    model.eval()
    total_loss, total_tokens = 0, 0
    progress_bar = tqdm(dataset, desc="Evaluating", unit="sample")
    
    with torch.no_grad():
        for i, example in enumerate(progress_bar):
            text = example["text"].strip()
            if not text:
                continue
                
            inputs = tokenizer(text, return_tensors="pt", truncation=True).to(device)
            outputs = model(**inputs, labels=inputs["input_ids"])
            
            total_loss += outputs.loss.item() * inputs["input_ids"].size(1)
            total_tokens += inputs["input_ids"].size(1)
            
            # 定期更新进度条
            if (i + 1) % Config.print_interval == 0:
                progress_bar.set_postfix({
                    "avg_ppl": math.exp(total_loss / total_tokens) if total_tokens > 0 else float('nan')
                })
    
    return math.exp(total_loss / total_tokens) if total_tokens > 0 else float('nan')

# ==================== 评估原始模型 ====================
def evaluate_original_models():
    """评估原始1B和3B模型的PPL（完整测试集）"""
    model_1b, model_3b, tokenizer = load_base_models()
    eval_dataset = load_dataset("wikitext", Config.eval_dataset, split="test")
    
    print("\nEvaluating Original 1B Model (Full Test Set)...")
    ppl_1b = evaluate_full_ppl(model_1b, tokenizer, eval_dataset)
    print(f"Original 1B PPL: {ppl_1b:.2f}")
    
    print("\nEvaluating Original 3B Model (Full Test Set)...")
    ppl_3b = evaluate_full_ppl(model_3b, tokenizer, eval_dataset)
    print(f"Original 3B PPL: {ppl_3b:.2f}")
    
    # 释放显存
    del model_1b, model_3b
    torch.cuda.empty_cache()
    
    return ppl_1b, ppl_3b

# ==================== Alpha优化流程 ====================
def find_optimal_alpha(ppl_1b, ppl_3b):
    # 1. 加载基础组件
    model_1b, model_3b, tokenizer = load_base_models()
    eval_dataset = load_dataset("wikitext", Config.eval_dataset, split="test")
    
    # 2. 准备嵌入层
    emb_1b = get_normalized_embeddings(model_1b)
    emb_3b = get_normalized_embeddings(model_3b)
    
    # 3. 加载投影层
    projector = TokenProjector(emb_3b.size(1), emb_1b.size(1)).to(device)
    projector.load_state_dict(torch.load(Config.projector_path))
    projector.eval()
    
    # 4. 投影3B嵌入
    with torch.no_grad():
        projected_3b = projector(emb_3b)
    
    # 5. 测试不同alpha
    results = {
        "Original 1B": ppl_1b,
        "Original 3B": ppl_3b
    }
    
    for alpha in Config.test_alphas:
        print(f"\nEvaluating alpha={alpha:.1f} (Full Test Set)...")
        
        # 融合嵌入
        combined_emb = alpha * emb_1b + (1 - alpha) * projected_3b
        combined_emb = F.normalize(combined_emb, p=2, dim=1)
        
        # 临时替换1B模型的嵌入层
        original_weights = model_1b.model.embed_tokens.weight.data.clone()
        model_1b.model.embed_tokens.weight = nn.Parameter(combined_emb.to(torch.float16))
        
        # 评估
        ppl = evaluate_full_ppl(model_1b, tokenizer, eval_dataset)
        results[f"Alpha={alpha:.1f}"] = ppl
        print(f"Alpha={alpha:.1f}, PPL={ppl:.2f}")
        
        # 恢复原始权重
        model_1b.model.embed_tokens.weight = nn.Parameter(original_weights)
        torch.cuda.empty_cache()
    
    # 6. 选择最佳alpha
    best_alpha = min(
        [(k,v) for k,v in results.items() if k.startswith("Alpha")], 
        key=lambda x: x[1]
    )[0]
    best_ppl = results[best_alpha]
    print(f"\n=== Optimal {best_alpha} (PPL={best_ppl:.2f}) ===")
    
    # 可视化结果
    print("\n=== Performance Summary (Full Test Set) ===")
    for name in ["Original 1B", "Original 3B"] + [f"Alpha={a:.1f}" for a in Config.test_alphas]:
        print(f"{name:<15}: PPL={results.get(name, 'N/A'):.2f}")
    
    return best_alpha, results

# ==================== 执行流程 ====================
if __name__ == "__main__":
    # 步骤1：评估原始模型（完整测试集）
    ppl_1b, ppl_3b = evaluate_original_models()
    
    # 步骤2：寻找最佳alpha（完整测试集）
    best_alpha, all_results = find_optimal_alpha(ppl_1b, ppl_3b)
    
    # 步骤3：使用最佳alpha进行最终融合
    if best_alpha.startswith("Alpha="):
        alpha_value = float(best_alpha.split("=")[1])
        model_1b, model_3b, tokenizer = load_base_models()
        emb_1b = get_normalized_embeddings(model_1b)
        emb_3b = get_normalized_embeddings(model_3b)
        
        projector = TokenProjector(emb_3b.size(1), emb_1b.size(1)).to(device)
        projector.load_state_dict(torch.load(Config.projector_path))
        projector.eval()
        
        with torch.no_grad():
            projected_3b = projector(emb_3b)
            combined_emb = alpha_value * emb_1b + (1 - alpha_value) * projected_3b
            combined_emb = F.normalize(combined_emb, p=2, dim=1)
            
            model_1b.model.embed_tokens.weight = nn.Parameter(combined_emb.to(torch.float16))
            output_path = f"{Config.output_dir}/best_merged_alpha{alpha_value:.1f}"
            model_1b.save_pretrained(output_path)
            tokenizer.save_pretrained(output_path)
        
        print(f"\nFinal model saved to {output_path} with {best_alpha}")